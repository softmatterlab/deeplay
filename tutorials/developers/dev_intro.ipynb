{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction for Developers\n",
    "\n",
    "This notebook presents a minimal example of how to implement a neural network and an application with deeplay.\n",
    "Specifically, it implements the classes for a multilayer perceptron and a classifier.\n",
    "Then, it combines them to demonstrate how they can be used for a simple classification task.\n",
    "Finally, it upgrades these classes adding functionalities that are required to improved the user experience when using an IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Multilayer Perceptron\n",
    "\n",
    "Here, we implement the minimal class `SimpleMLP`. It extends directly `dl.DeeplayModule`, which is the base class for all modules in `deeplay`.\n",
    "\n",
    "It represents a multilayer perceptron with a certain umber of inputs (`ìn_features`, which is an integer), a series of hidden layers with a certain number of neurons (`hidden_features`, a vector with the number of neurons for each layer), and a certain number of outputs (`out_features`, which is an integer).\n",
    "\n",
    "The constructor initializes the MLP by creating a sequence of linear and ReLU activation layers.\n",
    "\n",
    "The `forward` method defines the data flow through the network, sequentially passing the input through each linear-activation block and returning the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.blocks = dl.LayerList()\n",
    "        for inputs_layer, outputs_layer in zip([in_features, *hidden_features], \n",
    "                                               [*hidden_features, out_features]):\n",
    "            self.blocks.append(\n",
    "                dl.LayerActivationBlock(\n",
    "                    dl.Layer(nn.Linear, inputs_layer, outputs_layer),\n",
    "                    dl.Layer(nn.ReLU)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an instance of `SimpleMLP` in various ways, for example:\n",
    "\n",
    "```python\n",
    "mlp = SimpleMLP(2, [32, 32], 2)\n",
    "```\n",
    "\n",
    "or more explicitly:\n",
    "\n",
    "```python\n",
    "mlp = SimpleMLP(\n",
    "    in_features=2, \n",
    "    hidden_feature=[32, 32], \n",
    "    out_features=2\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (blocks): LayerList(\n",
      "    (0): LayerActivationBlock(\n",
      "      (layer): Layer[Linear](in_features=2, out_features=32)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "    (1): LayerActivationBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=32)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "    (2): LayerActivationBlock(\n",
      "      (layer): Layer[Linear](in_features=32, out_features=2)\n",
      "      (activation): Layer[ReLU]()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mlp = SimpleMLP(2, [32, 32], 2)\n",
    "\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Classifier\n",
    "\n",
    "Here, we now implement the application `SimpleClassifier`. This extend the deeplay class `dl.Application`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(dl.Application):\n",
    "\n",
    "    def __init__(self, model, **kwargs):\n",
    "        self.model = model\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an instance of this using as model the `mlp` that we have defined above, setting `loss` to `nn.CrossEntropyLoss()` and `optimizer`to `dl.Adam(lr=1e-3)`. We also add kepp track of a metrics, setting `metrics` to `[tm.Accuracy(\"multiclass\", num_classes=2)]`.\n",
    "\n",
    "Since we are using a cross-entropy loss, we need to set the output activation to `nn.Identity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleClassifier(\n",
      "  (model): SimpleMLP(\n",
      "    (blocks): LayerList(\n",
      "      (0): LayerActivationBlock(\n",
      "        (layer): Linear(in_features=2, out_features=32, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): LayerActivationBlock(\n",
      "        (layer): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): LayerActivationBlock(\n",
      "        (layer): Linear(in_features=32, out_features=2, bias=True)\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (optimizer): Adam[Adam](lr=0.001, params=<generator object Module.parameters at 0x7ff099b838b0>)\n",
      "  (train_metrics): MetricCollection(\n",
      "    (MulticlassAccuracy): MulticlassAccuracy(),\n",
      "    prefix=train\n",
      "  )\n",
      "  (val_metrics): MetricCollection(\n",
      "    (MulticlassAccuracy): MulticlassAccuracy(),\n",
      "    prefix=val\n",
      "  )\n",
      "  (test_metrics): MetricCollection(\n",
      "    (MulticlassAccuracy): MulticlassAccuracy(),\n",
      "    prefix=test\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics as tm\n",
    "\n",
    "classifier = SimpleClassifier(\n",
    "    model=mlp, \n",
    "    loss=nn.CrossEntropyLoss(), \n",
    "    optimizer=dl.Adam(lr=1e-3), \n",
    "    metrics=[tm.Accuracy(\"multiclass\", num_classes=2)]\n",
    ")\n",
    "classifier.model.blocks[-1].activation.configure(nn.Identity)\n",
    "\n",
    "classifier.build()\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "Instead of `classifier.build()`, which build the module in place, it is also possible to use `new_classifier = classifier.create()`, which clones and build the classifier.\n",
    "\n",
    "Instead of `classifier.model.blocks[-1].activation.configure(nn.Identity)`, it'd also be possible to use `classifier.model.output.activation.configure(nn.Identity)`, which is more easily understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We'll now use `classifier` for the simple task of determining whether the sum of two numbers is larger or smaller than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/giovannivolpe/Documents/GitHub/deeplay/tutorials/developers/lightning_logs\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | SimpleMLP        | 1.2 K \n",
      "1 | loss          | CrossEntropyLoss | 0     \n",
      "2 | train_metrics | MetricCollection | 0     \n",
      "3 | val_metrics   | MetricCollection | 0     \n",
      "4 | test_metrics  | MetricCollection | 0     \n",
      "5 | optimizer     | Adam             | 0     \n",
      "---------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 50.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/miniconda3/envs/py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/miniconda3/envs/py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/giovannivolpe/miniconda3/envs/py310/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 50.71it/s, v_num=0, train_loss_step=0.501, trainMulticlassAccuracy_step=1.000, val_loss_step=0.560, valMulticlassAccuracy_step=0.750, val_loss_epoch=0.513, valMulticlassAccuracy_epoch=0.900, train_loss_epoch=0.496, trainMulticlassAccuracy_epoch=0.962] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00, 45.62it/s, v_num=0, train_loss_step=0.501, trainMulticlassAccuracy_step=1.000, val_loss_step=0.560, valMulticlassAccuracy_step=0.750, val_loss_epoch=0.513, valMulticlassAccuracy_epoch=0.900, train_loss_epoch=0.496, trainMulticlassAccuracy_epoch=0.962]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannivolpe/miniconda3/envs/py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 73.47it/s] \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "testMulticlassAccuracy_epoch     0.8999999761581421\n",
      "      test_loss_epoch            0.5125035047531128\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.5125035047531128,\n",
       "  'testMulticlassAccuracy_epoch': 0.8999999761581421}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import randn\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "num_samples = 100\n",
    "data = randn(num_samples, 2)\n",
    "labels = (data.sum(dim=1) > 0).long()\n",
    "\n",
    "dataset = TensorDataset(data, labels)\n",
    "train, val = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val, batch_size=16, shuffle=False)\n",
    "\n",
    "trainer = dl.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(classifier, train_dataloader, val_dataloader)\n",
    "\n",
    "trainer.test(classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-of-Life Improvements for IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements to `SimpleMLP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by commenting the class, adding:\n",
    "\n",
    "````python\n",
    "#---snip---\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "    \"\"\"Multi-layer perceptron module.\n",
    "\n",
    "    Also commonly known as a fully-connected neural network, or a dense neural network.\n",
    "\n",
    "    Configurables\n",
    "    -------------\n",
    "    - in_features (int): Number of input features. If None, the input shape is inferred in the first forward pass. (Default: None)\n",
    "    - hidden_features (list[int]): Number of hidden units in each layer.\n",
    "    - out_features (int): Number of output features. (Default: 1)\n",
    "    - blocks (template-like): Specification for the blocks of the MLP. (Default: \"layer\" >> \"activation\" >> \"normalization\" >> \"dropout\")\n",
    "        - layer (template-like): Specification for the layer of the block. (Default: nn.Linear)\n",
    "        - activation (template-like): Specification for the activation of the block. (Default: nn.ReLU)\n",
    "        - normalization (template-like): Specification for the normalization of the block. (Default: nn.Identity)\n",
    "    - out_activation (template-like): Specification for the output activation of the MLP. (Default: nn.Identity)\n",
    "\n",
    "    Evaluation\n",
    "    ----------\n",
    "    >>> for block in mlp.blocks:\n",
    "    >>>    x = block(x)\n",
    "    >>> return x\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> mlp = SimpleMLP(28 * 28, [128, 128], 10)\n",
    "    >>> mlp.build()\n",
    "\n",
    "    Return Values\n",
    "    -------------\n",
    "    The forward method returns the processed tensor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #---snip---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then add some defaults for the input parameters:\n",
    "\n",
    "```python\n",
    "#---snip---\n",
    "from typing import Optional, Sequence, Type\n",
    "\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "    #---snip---\n",
    "\n",
    "    in_features: Optional[int]\n",
    "    hidden_features: Sequence[Optional[int]]\n",
    "    out_features: int\n",
    "    blocks: dl.LayerList[dl.blocks.LAN.LayerActivationNormalizationBlock]\n",
    "\n",
    "    #---snip---\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: Optional[int],\n",
    "        hidden_features: Sequence[Optional[int]],\n",
    "        out_features: int,\n",
    "    ):\n",
    "        #---snip---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then add some checks on the input parameters:\n",
    "\n",
    "```python\n",
    "#---snip---\n",
    "from typing import Optional, Sequence, Type\n",
    "\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "    #---snip---\n",
    "\n",
    "    def __init__(\n",
    "        #---snip---\n",
    "    ):\n",
    "        #---snip---\n",
    "        \n",
    "        if out_features <= 0:\n",
    "            raise ValueError(\n",
    "                f\"Number of output features must be positive, got {out_features}\"\n",
    "            )\n",
    "\n",
    "        if in_features is not None and in_features <= 0:\n",
    "            raise ValueError(f\"in_channels must be positive, got {in_features}\")\n",
    "\n",
    "        if any(h <= 0 for h in hidden_features):\n",
    "            raise ValueError(\n",
    "                f\"all hidden_channels must be positive, got {hidden_features}\"\n",
    "            )\n",
    "        \n",
    "    #---snip---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then add some shorthands and the relative documentation for convenience:\n",
    "\n",
    "```python\n",
    "#---snip---\n",
    "\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "    \"\"\"Multi-layer perceptron module.\n",
    "    ---snip---\n",
    "\n",
    "    Shorthands\n",
    "    ----------\n",
    "    - `input`: Equivalent to `.blocks[0]`.\n",
    "    - `hidden`: Equivalent to `.blocks[:-1]`.\n",
    "    - `output`: Equivalent to `.blocks[-1]`.\n",
    "    - `layer`: Equivalent to `.blocks.layer`.\n",
    "    - `activation`: Equivalent to `.blocks.activation`.\n",
    "    - `normalization`: Equivalent to `.blocks.normalization`.\n",
    "\n",
    "    ---snip---\n",
    "    \"\"\"\n",
    "\n",
    "    #---snip---\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        \"\"\"Return the input layer of the network. Equivalent to `.blocks[0]`.\"\"\"\n",
    "        return self.blocks[0]\n",
    "\n",
    "    @property\n",
    "    def hidden(self):\n",
    "        \"\"\"Return the hidden layers of the network. Equivalent to `.blocks[:-1]`\"\"\"\n",
    "        return self.blocks[:-1]\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        \"\"\"Return the last layer of the network. Equivalent to `.blocks[-1]`.\"\"\"\n",
    "        return self.blocks[-1]\n",
    "\n",
    "    @property\n",
    "    def layer(self) -> LayerList[Layer]:\n",
    "        \"\"\"Return the layers of the network. Equivalent to `.blocks.layer`.\"\"\"\n",
    "        return self.blocks.layer\n",
    "\n",
    "    @property\n",
    "    def activation(self) -> LayerList[Layer]:\n",
    "        \"\"\"Return the activations of the network. Equivalent to `.blocks.activation`.\"\"\"\n",
    "        return self.blocks.activation\n",
    "\n",
    "    @property\n",
    "    def normalization(self) -> LayerList[Layer]:\n",
    "        \"\"\"Return the normalizations of the network. Equivalent to `.blocks.normalization`.\"\"\"\n",
    "        return self.blocks.normalization\n",
    "\n",
    "    #---snip---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add some default configurations for the IDE:\n",
    "\n",
    "```python\n",
    "#---snip---\n",
    "from typing import overload, List, Literal, Any\n",
    "\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "    #---snip---\n",
    "\n",
    "    @overload\n",
    "    def configure(\n",
    "        self,\n",
    "        /,\n",
    "        in_features: int | None = None,\n",
    "        hidden_features: List[int] | None = None,\n",
    "        out_features: int | None = None,\n",
    "        out_activation: Type[nn.Module] | nn.Module | None = None,\n",
    "    ) -> None:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def configure(\n",
    "        self,\n",
    "        name: Literal[\"blocks\"],\n",
    "        index: int | slice | List[int | slice] | None = None,\n",
    "        order: Optional[Sequence[str]] = None,\n",
    "        layer: Optional[Type[nn.Module]] = None,\n",
    "        activation: Optional[Type[nn.Module]] = None,\n",
    "        normalization: Optional[Type[nn.Module]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        ...\n",
    "\n",
    "    configure = dl.DeeplayModule.configure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Sequence, Type, overload, List, Literal, Any\n",
    "\n",
    "\n",
    "class SimpleMLP(dl.DeeplayModule):\n",
    "    \"\"\"Multi-layer perceptron module.\n",
    "\n",
    "    Also commonly known as a fully-connected neural network, or a dense neural network.\n",
    "\n",
    "    Configurables\n",
    "    -------------\n",
    "    - in_features (int): Number of input features. If None, the input shape is inferred in the first forward pass. (Default: None)\n",
    "    - hidden_features (list[int]): Number of hidden units in each layer.\n",
    "    - out_features (int): Number of output features. (Default: 1)\n",
    "    - blocks (template-like): Specification for the blocks of the MLP. (Default: \"layer\" >> \"activation\" >> \"normalization\" >> \"dropout\")\n",
    "        - layer (template-like): Specification for the layer of the block. (Default: nn.Linear)\n",
    "        - activation (template-like): Specification for the activation of the block. (Default: nn.ReLU)\n",
    "        - normalization (template-like): Specification for the normalization of the block. (Default: nn.Identity)\n",
    "\n",
    "    Evaluation\n",
    "    ----------\n",
    "    >>> for block in mlp.blocks:\n",
    "    >>>    x = block(x)\n",
    "    >>> return x\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> mlp = SimpleMLP(28 * 28, [128, 128], 10)\n",
    "    >>> mlp.build()\n",
    "\n",
    "    Shorthands\n",
    "    ----------\n",
    "    - `input`: Equivalent to `.blocks[0]`.\n",
    "    - `hidden`: Equivalent to `.blocks[:-1]`.\n",
    "    - `output`: Equivalent to `.blocks[-1]`.\n",
    "    - `layer`: Equivalent to `.blocks.layer`.\n",
    "    - `activation`: Equivalent to `.blocks.activation`.\n",
    "    - `normalization`: Equivalent to `.blocks.normalization`.    \n",
    "\n",
    "    Return Values\n",
    "    -------------\n",
    "    The forward method returns the processed tensor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    in_features: Optional[int]\n",
    "    hidden_features: Sequence[Optional[int]]\n",
    "    out_features: int\n",
    "    blocks: dl.LayerList[dl.LayerActivationBlock]\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        \"\"\"Return the input layer of the network. Equivalent to `.blocks[0]`.\"\"\"\n",
    "        return self.blocks[0]\n",
    "\n",
    "    @property\n",
    "    def hidden(self):\n",
    "        \"\"\"Return the hidden layers of the network. Equivalent to `.blocks[:-1]`\"\"\"\n",
    "        return self.blocks[:-1]\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        \"\"\"Return the last layer of the network. Equivalent to `.blocks[-1]`.\"\"\"\n",
    "        return self.blocks[-1]\n",
    "\n",
    "    @property\n",
    "    def layer(self) -> dl.LayerList[dl.Layer]:\n",
    "        \"\"\"Return the layers of the network. Equivalent to `.blocks.layer`.\"\"\"\n",
    "        return self.blocks.layer\n",
    "\n",
    "    @property\n",
    "    def activation(self) -> dl.LayerList[dl.Layer]:\n",
    "        \"\"\"Return the activations of the network. Equivalent to `.blocks.activation`.\"\"\"\n",
    "        return self.blocks.activation\n",
    "\n",
    "    @property\n",
    "    def normalization(self) -> dl.LayerList[dl.Layer]:\n",
    "        \"\"\"Return the normalizations of the network. Equivalent to `.blocks.normalization`.\"\"\"\n",
    "        return self.blocks.normalization\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: Optional[int],\n",
    "        hidden_features: Sequence[Optional[int]],\n",
    "        out_features: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        if out_features <= 0:\n",
    "            raise ValueError(\n",
    "                f\"Number of output features must be positive, got {out_features}\"\n",
    "            )\n",
    "\n",
    "        if in_features is not None and in_features <= 0:\n",
    "            raise ValueError(f\"in_channels must be positive, got {in_features}\")\n",
    "\n",
    "        if any(h <= 0 for h in hidden_features):\n",
    "            raise ValueError(\n",
    "                f\"all hidden_channels must be positive, got {hidden_features}\"\n",
    "            )\n",
    "\n",
    "        self.blocks = dl.LayerList()\n",
    "        for inputs_layer, outputs_layer in zip([in_features, *hidden_features], \n",
    "                                               [*hidden_features, out_features]):\n",
    "            self.blocks.append(\n",
    "                dl.LayerActivationBlock(\n",
    "                    dl.Layer(nn.Linear, inputs_layer, outputs_layer),\n",
    "                    dl.Layer(nn.ReLU)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @overload\n",
    "    def configure(\n",
    "        self,\n",
    "        /,\n",
    "        in_features: int | None = None,\n",
    "        hidden_features: List[int] | None = None,\n",
    "        out_features: int | None = None,\n",
    "        out_activation: Type[nn.Module] | nn.Module | None = None,\n",
    "    ) -> None:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def configure(\n",
    "        self,\n",
    "        name: Literal[\"blocks\"],\n",
    "        index: int | slice | List[int | slice] | None = None,\n",
    "        order: Optional[Sequence[str]] = None,\n",
    "        layer: Optional[Type[nn.Module]] = None,\n",
    "        activation: Optional[Type[nn.Module]] = None,\n",
    "        normalization: Optional[Type[nn.Module]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        ...\n",
    "\n",
    "    configure = dl.DeeplayModule.configure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements to `SimpleClassifier`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
